{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbe250a-e8a9-4c51-9ce1-1c9d3e77a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://dmitriis-mbp:4040\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1670364470911)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6e86a6a3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa318a77-f037-40c6-bdf1-dca1858ccc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
      "|              id|        comment_text|toxic|severe_toxic|obscene|threat|insult|identity_hate|\n",
      "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
      "|0000997932d777bf|Explanation\\nWhy ...|    0|           0|      0|     0|     0|            0|\n",
      "|000103f0d9cfb60f|D'aww! He matches...|    0|           0|      0|     0|     0|            0|\n",
      "|000113f07ec002fd|Hey man, I'm real...|    0|           0|      0|     0|     0|            0|\n",
      "|0001b41b1c6bb37e|\"\\nMore\\nI can't ...|    0|           0|      0|     0|     0|            0|\n",
      "|0001d958c54c6e35|You, sir, are my ...|    0|           0|      0|     0|     0|            0|\n",
      "|00025465d4725e87|\"\\n\\nCongratulati...|    0|           0|      0|     0|     0|            0|\n",
      "|0002bcb3da6cb337|COCKSUCKER BEFORE...|    1|           1|      1|     0|     1|            0|\n",
      "|00031b1e95af7921|Your vandalism to...|    0|           0|      0|     0|     0|            0|\n",
      "|00037261f536c51d|Sorry if the word...|    0|           0|      0|     0|     0|            0|\n",
      "|00040093b2687caa|alignment on this...|    0|           0|      0|     0|     0|            0|\n",
      "|0005300084f90edc|\"\\nFair use ratio...|    0|           0|      0|     0|     0|            0|\n",
      "|00054a5e18b50dd4|bbq \\n\\nbe a man ...|    0|           0|      0|     0|     0|            0|\n",
      "|0005c987bdfc9d4b|Hey... what is it...|    1|           0|      0|     0|     0|            0|\n",
      "|0006f16e4e9f292e|Before you start ...|    0|           0|      0|     0|     0|            0|\n",
      "|00070ef96486d6f9|Oh, and the girl ...|    0|           0|      0|     0|     0|            0|\n",
      "|00078f8ce7eb276d|\"\\n\\nJuelz Santan...|    0|           0|      0|     0|     0|            0|\n",
      "|0007e25b2121310b|Bye! \\n\\nDon't lo...|    1|           0|      0|     0|     0|            0|\n",
      "|000897889268bc93|REDIRECT Talk:Voy...|    0|           0|      0|     0|     0|            0|\n",
      "|0009801bd85e5806|The Mitsurugi poi...|    0|           0|      0|     0|     0|            0|\n",
      "|0009eaea3325de8c|Don't mean to bot...|    0|           0|      0|     0|     0|            0|\n",
      "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, comment_text: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    ".read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"escape\",\"\\\"\")\n",
    ".option(\"multiLine\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"train.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e6adc7-9908-4be2-b845-a3d7659d2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- toxic: integer (nullable = true)\n",
      " |-- severe_toxic: integer (nullable = true)\n",
      " |-- obscene: integer (nullable = true)\n",
      " |-- threat: integer (nullable = true)\n",
      " |-- insult: integer (nullable = true)\n",
      " |-- identity_hate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c801c79-a088-4f6d-9038-f7f55e13fc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer, Word2Vec}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{TrainValidationSplit, ParamGridBuilder}\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer, Word2Vec}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{TrainValidationSplit, ParamGridBuilder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24239735-0a9c-4cc3-9254-91f65e215fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "targets: Array[String] = Array(toxic, severe_toxic, obscene, threat, insult, identity_hate)\n",
       "num_features: Array[Int] = Array(10, 100, 1000, 10000)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val targets = Array(\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\")\n",
    "val num_features = Array(10, 100, 1000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130bf1dc-9c19-41df-aa83-32cf43278fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 22:53:30 WARN BlockManager: Asked to remove block broadcast_5593_piece0, which does not exist\n",
      "target: toxic\n",
      "numFeatures: 10 - ROC-AUC score: 0.6395966506739984\n",
      "numFeatures: 100 - ROC-AUC score: 0.7808456528214043\n",
      "numFeatures: 1000 - ROC-AUC score: 0.8742418602472807\n",
      "numFeatures: 10000 - ROC-AUC score: 0.8720380596670918\n",
      "target: severe_toxic\n",
      "numFeatures: 10 - ROC-AUC score: 0.7150827845741294\n",
      "numFeatures: 100 - ROC-AUC score: 0.7959638071490243\n",
      "numFeatures: 1000 - ROC-AUC score: 0.8921820181480886\n",
      "numFeatures: 10000 - ROC-AUC score: 0.829040210962328\n",
      "target: obscene\n",
      "numFeatures: 10 - ROC-AUC score: 0.6659464636868121\n",
      "numFeatures: 100 - ROC-AUC score: 0.7917703491161217\n",
      "numFeatures: 1000 - ROC-AUC score: 0.8929038256643931\n",
      "numFeatures: 10000 - ROC-AUC score: 0.8502915385051808\n",
      "target: threat\n",
      "numFeatures: 10 - ROC-AUC score: 0.6764621165453284\n",
      "numFeatures: 100 - ROC-AUC score: 0.8060826214327531\n",
      "numFeatures: 1000 - ROC-AUC score: 0.868782423416597\n",
      "numFeatures: 10000 - ROC-AUC score: 0.8296726972749398\n",
      "target: insult\n",
      "numFeatures: 10 - ROC-AUC score: 0.6707439600718667\n",
      "numFeatures: 100 - ROC-AUC score: 0.8069225145419606\n",
      "numFeatures: 1000 - ROC-AUC score: 0.8900116762075657\n",
      "numFeatures: 10000 - ROC-AUC score: 0.8589895241314807\n",
      "target: identity_hate\n",
      "numFeatures: 10 - ROC-AUC score: 0.614093369157778\n",
      "numFeatures: 100 - ROC-AUC score: 0.7353108079966713\n",
      "numFeatures: 1000 - ROC-AUC score: 0.8354776316027757\n",
      "numFeatures: 10000 - ROC-AUC score: 0.7665215919997114\n"
     ]
    }
   ],
   "source": [
    "for (target <- targets) {\n",
    "    val tokenizer = new Tokenizer()\n",
    "        .setInputCol(\"comment_text\")\n",
    "        .setOutputCol(\"words\")\n",
    "    val hashingTF = new HashingTF()\n",
    "        .setNumFeatures(1000)\n",
    "        .setInputCol(tokenizer.getOutputCol)\n",
    "        .setOutputCol(\"rawFeatures\")\n",
    "    val idf = new IDF()\n",
    "        .setInputCol(hashingTF.getOutputCol)\n",
    "        .setOutputCol(\"features\")\n",
    "    val lr = new LogisticRegression()\n",
    "        .setLabelCol(target)\n",
    "    val pipeline = new Pipeline()\n",
    "        .setStages(\n",
    "            Array(\n",
    "                tokenizer,\n",
    "                hashingTF,\n",
    "                idf,\n",
    "                lr\n",
    "            )\n",
    "        )\n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "        .addGrid(hashingTF.numFeatures, num_features)\n",
    "        .build()\n",
    "\n",
    "    val evaluator = new BinaryClassificationEvaluator()\n",
    "        .setLabelCol(target)\n",
    "\n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "        .setEstimator(pipeline)\n",
    "        .setEvaluator(evaluator)\n",
    "        .setEstimatorParamMaps(paramGrid)\n",
    "        .setTrainRatio(0.8)\n",
    "        .setParallelism(4)\n",
    "    val model = trainValidationSplit.fit(df)\n",
    "\n",
    "    println(s\"target: ${target}\")\n",
    "    for (i <- 0 until num_features.length) {\n",
    "        println(s\"numFeatures: ${num_features(i)} - ROC-AUC score: ${model.validationMetrics(i)}\")\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8b77e-f161-45ee-bab7-e15e7ac75b1f",
   "metadata": {},
   "source": [
    "**the more numFeatures the better the result, but starting from a certain point, our model starts to overfit due to the large complexity caused by numFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10adbbd5-b065-472f-b805-ddbddcaaba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_8d49520d6eb2\n",
       "word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_05495579dd52\n",
       "w2v_pipeline: org.apache.spark.ml.Pipeline = pipeline_05812102f229\n",
       "w2v_transormation: org.apache.spark.ml.PipelineModel = pipeline_05812102f229\n",
       "w2v_df: org.apache.spark.sql.DataFrame = [id: string, comment_text: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"comment_text\")\n",
    "    .setOutputCol(\"words\")\n",
    "val word2Vec = new Word2Vec()\n",
    "    .setInputCol(\"words\")\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val w2v_pipeline = new Pipeline()\n",
    "    .setStages(\n",
    "        Array(\n",
    "            tokenizer,\n",
    "            word2Vec,\n",
    "        )\n",
    "    )\n",
    "\n",
    "val w2v_transormation = w2v_pipeline.fit(df)\n",
    "\n",
    "val w2v_df = w2v_transormation.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1acbd1c4-380f-4d62-a1a8-59976dbfaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 23:16:02 WARN MemoryStore: Not enough space to cache rdd_39_0 in memory! (computed 258.1 MiB so far)\n",
      "22/12/06 23:16:02 WARN BlockManager: Persisting block rdd_39_0 to disk instead.\n",
      "22/12/06 23:16:10 WARN MemoryStore: Not enough space to cache rdd_125_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:16:10 WARN BlockManager: Persisting block rdd_125_0 to disk instead.\n",
      "target: toxic\n",
      "W2V - ROC-AUC score: 0.9407561243442133\n",
      "22/12/06 23:16:26 WARN MemoryStore: Not enough space to cache rdd_226_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:16:26 WARN BlockManager: Persisting block rdd_226_0 to disk instead.\n",
      "22/12/06 23:16:35 WARN MemoryStore: Not enough space to cache rdd_327_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:16:35 WARN BlockManager: Persisting block rdd_327_0 to disk instead.\n",
      "target: severe_toxic\n",
      "W2V - ROC-AUC score: 0.9697990140702598\n",
      "22/12/06 23:16:50 WARN MemoryStore: Not enough space to cache rdd_449_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:16:50 WARN BlockManager: Persisting block rdd_449_0 to disk instead.\n",
      "22/12/06 23:16:59 WARN MemoryStore: Not enough space to cache rdd_537_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:16:59 WARN BlockManager: Persisting block rdd_537_0 to disk instead.\n",
      "target: obscene\n",
      "W2V - ROC-AUC score: 0.9463978116438774\n",
      "22/12/06 23:17:14 WARN MemoryStore: Not enough space to cache rdd_640_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:17:14 WARN BlockManager: Persisting block rdd_640_0 to disk instead.\n",
      "22/12/06 23:17:23 WARN MemoryStore: Not enough space to cache rdd_743_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:17:23 WARN BlockManager: Persisting block rdd_743_0 to disk instead.\n",
      "target: threat\n",
      "W2V - ROC-AUC score: 0.9538135775481328\n",
      "22/12/06 23:17:39 WARN MemoryStore: Not enough space to cache rdd_863_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:17:39 WARN BlockManager: Persisting block rdd_863_0 to disk instead.\n",
      "22/12/06 23:17:47 WARN MemoryStore: Not enough space to cache rdd_952_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:17:47 WARN BlockManager: Persisting block rdd_952_0 to disk instead.\n",
      "target: insult\n",
      "W2V - ROC-AUC score: 0.9457034184174289\n",
      "22/12/06 23:18:02 WARN MemoryStore: Not enough space to cache rdd_1056_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:18:02 WARN BlockManager: Persisting block rdd_1056_0 to disk instead.\n",
      "22/12/06 23:18:10 WARN MemoryStore: Not enough space to cache rdd_1155_0 in memory! (computed 67.5 MiB so far)\n",
      "22/12/06 23:18:10 WARN BlockManager: Persisting block rdd_1155_0 to disk instead.\n",
      "target: identity_hate\n",
      "W2V - ROC-AUC score: 0.9335569531716756\n"
     ]
    }
   ],
   "source": [
    "for (target <- targets) {\n",
    "    val lr = new LogisticRegression()\n",
    "        .setLabelCol(target)\n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "        .build()\n",
    "    val evaluator = new BinaryClassificationEvaluator()\n",
    "        .setLabelCol(target)\n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "        .setEstimator(lr)\n",
    "        .setEvaluator(evaluator)\n",
    "        .setEstimatorParamMaps(paramGrid)\n",
    "        .setTrainRatio(0.8)\n",
    "    val model = trainValidationSplit.fit(w2v_df)\n",
    "\n",
    "    println(s\"target: ${target}\")\n",
    "    println(s\"W2V - ROC-AUC score: ${model.validationMetrics(0)}\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734f3e1-0bed-4d0c-b1ca-c324eacf884f",
   "metadata": {},
   "source": [
    "**LogisticRegression with W2V works much better than with TFIDF**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
